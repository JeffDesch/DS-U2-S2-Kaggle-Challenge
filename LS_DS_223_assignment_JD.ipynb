{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiOBDOv02Dn-"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 3*\n",
        "\n",
        "---\n",
        "<p style=\"padding: 10px; border: 2px solid red;\">\n",
        "    <b>Before you start:</b> Today is the day you should submit the dataset for your Unit 2 Build Week project. You can review the guidelines and make your submission in the Build Week course for your cohort on Canvas.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oQ6Tm_HQ2DoD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/main/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "    !pip install pandas-profiling==2.*\n",
        "    \n",
        "    #Connect to remote data\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    %cd /content/drive/My Drive/Kaggle\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YXzxH382DoF"
      },
      "source": [
        "# Module Project: Hyperparameter Tuning\n",
        "\n",
        "This sprint, the module projects will focus on creating and improving a model for the Tanazania Water Pump dataset. Your goal is to create a model to predict whether a water pump is functional, non-functional, or needs repair.\n",
        "\n",
        "Dataset source: [DrivenData.org](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/).\n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are as follows:\n",
        "\n",
        "- **Task 1:** Use `wrangle` function to import training and test data.\n",
        "- **Task 2:** Split training data into feature matrix `X` and target vector `y`.\n",
        "- **Task 3:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 4:** Build `clf_dt`.\n",
        "- **Task 5:** Build `clf_rf`.\n",
        "- **Task 6:** Evaluate classifiers using k-fold cross-validation.\n",
        "- **Task 7:** Tune hyperparameters for best performing classifier.\n",
        "- **Task 8:** Print out best score and params for model.\n",
        "- **Task 9:** Create `submission.csv` and upload to Kaggle.\n",
        "\n",
        "You should limit yourself to the following libraries for this project:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `pandas-profiling`\n",
        "- `sklearn`\n",
        "\n",
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P6BDf0Vi6Yab"
      },
      "outputs": [],
      "source": [
        "# Import Block\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iw39E9Po2DoG"
      },
      "outputs": [],
      "source": [
        "def wrangle(fm_path, tv_path=None):\n",
        "    if tv_path:\n",
        "        df = pd.merge(pd.read_csv(fm_path, \n",
        "                                  na_values=[0, -2.000000e-08]),\n",
        "                      pd.read_csv(tv_path)).set_index('id')\n",
        "    else:\n",
        "        df = pd.read_csv(fm_path, \n",
        "                         na_values=[0, -2.000000e-08],\n",
        "                         index_col='id')\n",
        "        \n",
        "    # Build new Pump Age feature\n",
        "    df['pump_age'] = pd.to_datetime(df['date_recorded']).dt.year - pd.to_datetime(df['construction_year']).dt.year\n",
        "    df.drop(columns=['date_recorded'], inplace=True)\n",
        "\n",
        "    # Drop constant columns\n",
        "    df.drop(columns=['recorded_by'], inplace=True)\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    #automatic method is too unreliable\n",
        "    #dupe_cols = [col for col in df.head(50).T.duplicated().index\n",
        "    #             if df.head(50).T.duplicated()[col]]\n",
        "    dupe_cols = ['subvillage', 'region', 'extraction_type_group', 'payment', \n",
        "                 'quality_group', 'quantity_group', 'source_type', 'waterpoint_type_group']\n",
        "    df.drop(columns=dupe_cols, inplace=True)    \n",
        "\n",
        "    # Drop HCCCs\n",
        "    cutoff = 100\n",
        "    drop_cols = [col for col in df.select_dtypes('object').columns\n",
        "                 if df[col].nunique() > cutoff]\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WextdWwj2DoH"
      },
      "source": [
        "**Task 1:** Using the above `wrangle` function to read `train_features.csv` and `train_labels.csv` into the DataFrame `df`, and `test_features.csv` into the DataFrame `X_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lrTPVGzm2DoH"
      },
      "outputs": [],
      "source": [
        "df = wrangle(\"train_features.csv\", \"train_labels.csv\")\n",
        "X_test = wrangle(\"test_features.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laqzBPCn2DoI"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 2:** Split your DataFrame `df` into a feature matrix `X` and the target vector `y`. You want to predict `'status_group'`.\n",
        "\n",
        "**Note:** You won't need to do a train-test split because you'll use cross-validation instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VdsdSXH32DoJ"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['status_group'])\n",
        "y = df['status_group']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxkkocKa2DoJ"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 3:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwdySjeI2DoK",
        "outputId": "3f2af8a5-bfa6-4f50-845b-adb120ee51b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Accuracy Score: 0.5429828068772491\n"
          ]
        }
      ],
      "source": [
        "model_dum = DummyClassifier(strategy='prior').fit(X, y)\n",
        "baseline_acc = accuracy_score(y, model_dum.predict(X))\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roI0AEX82DoL"
      },
      "source": [
        "# IV. Build Models\n",
        "\n",
        "**Task 4:** Build a `Pipeline` named `clf_dt`. Your `Pipeline` should include:\n",
        "\n",
        "- an `OrdinalEncoder` transformer for categorical features.\n",
        "- a `SimpleImputer` transformer fot missing values.\n",
        "- a `DecisionTreeClassifier` Predictor.\n",
        "\n",
        "**Note:** Do not train `clf_dt`. You'll do that in a subsequent task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5BmgNi-M2DoL"
      },
      "outputs": [],
      "source": [
        "clf_dt = make_pipeline(OrdinalEncoder(), SimpleImputer(), DecisionTreeClassifier(random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cnkYztb2DoM"
      },
      "source": [
        "**Task 5:** Build a `Pipeline` named `clf_rf`. Your `Pipeline` should include:\n",
        "\n",
        "- an `OrdinalEncoder` transformer for categorical features.\n",
        "- a `SimpleImputer` transformer fot missing values.\n",
        "- a `RandomForestClassifier` predictor.\n",
        "\n",
        "**Note:** Do not train `clf_rf`. You'll do that in a subsequent task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cYdG7en82DoM"
      },
      "outputs": [],
      "source": [
        "clf_rf = make_pipeline(OrdinalEncoder(), SimpleImputer(), RandomForestClassifier(random_state=42, n_jobs=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBXiiFey2DoN"
      },
      "source": [
        "# V. Check Metrics\n",
        "\n",
        "**Task 6:** Evaluate the performance of both of your classifiers using k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "46HsNTF_2DoN"
      },
      "outputs": [],
      "source": [
        "kfold_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_scores_dt = cross_val_score(clf_dt, X, y, cv=kfold_cv, scoring='accuracy')\n",
        "cv_scores_rf = cross_val_score(clf_rf, X, y, cv=kfold_cv, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO1AXHLv2DoN",
        "outputId": "69b4b51a-aadd-4a57-b398-d55579a12b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV scores DecisionTreeClassifier\n",
            "[0.74526515 0.74368687 0.75252525 0.75284091 0.75260444]\n",
            "Mean CV accuracy score: 0.7493845245042235\n",
            "STD CV accuracy score: 0.004040077793211837\n"
          ]
        }
      ],
      "source": [
        "print('CV scores DecisionTreeClassifier')\n",
        "print(cv_scores_dt)\n",
        "print('Mean CV accuracy score:', cv_scores_dt.mean())\n",
        "print('STD CV accuracy score:', cv_scores_dt.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pthDLpP22DoO",
        "outputId": "9f2d0dd1-874f-47d0-ac0a-2ed5192425d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV score RandomForestClassifier\n",
            "[0.79261364 0.79987374 0.79997896 0.80155724 0.80300958]\n",
            "Mean CV accuracy score: 0.7994066289893924\n",
            "STD CV accuracy score: 0.0035859963134000335\n"
          ]
        }
      ],
      "source": [
        "print('CV score RandomForestClassifier')\n",
        "print(cv_scores_rf)\n",
        "print('Mean CV accuracy score:', cv_scores_rf.mean())\n",
        "print('STD CV accuracy score:', cv_scores_rf.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLozw2xK2DoO"
      },
      "source": [
        "# VI. Tune Model\n",
        "\n",
        "**Task 7:** Choose the best performing of your two models and tune its hyperparameters using a `RandomizedSearchCV` named `model`. Make sure that you include cross-validation and that `n_iter` is set to at least `25`.\n",
        "\n",
        "**Note:** If you're not sure which hyperparameters to tune, check the notes from today's guided project and the `sklearn` documentation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "M3qYii6N2DoO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5,\n",
              "                   estimator=Pipeline(steps=[('ordinalencoder',\n",
              "                                              OrdinalEncoder()),\n",
              "                                             ('simpleimputer', SimpleImputer()),\n",
              "                                             ('randomforestclassifier',\n",
              "                                              RandomForestClassifier(n_jobs=-1,\n",
              "                                                                     random_state=42))]),\n",
              "                   n_iter=100, n_jobs=-1,\n",
              "                   param_distributions={'randomforestclassifier__criterion': ['gini',\n",
              "                                                                              'entropy'],\n",
              "                                        'randomforestclassifier__max_depth': array([30, 31, 32, 33...\n",
              "                                        'randomforestclassifier__max_leaf_nodes': array([2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400]),\n",
              "                                        'randomforestclassifier__min_samples_leaf': array([1, 2, 3, 4, 5, 6, 7]),\n",
              "                                        'randomforestclassifier__min_samples_split': array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),\n",
              "                                        'randomforestclassifier__n_estimators': array([65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n",
              "                                        'simpleimputer__strategy': ['mean',\n",
              "                                                                    'median']},\n",
              "                   random_state=42, scoring='accuracy', verbose=1)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_rf = make_pipeline(OrdinalEncoder(), SimpleImputer(), RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "\n",
        "param_rngs = {\"simpleimputer__strategy\": ['mean', 'median'],\n",
        "              \"randomforestclassifier__max_depth\": np.arange(30, 40, 1),\n",
        "              \"randomforestclassifier__max_features\": np.linspace(0.4, 0.5, 20),\n",
        "              \"randomforestclassifier__n_estimators\": np.arange(65, 75, 1),\n",
        "              \"randomforestclassifier__min_samples_split\": np.arange(4, 16, 1),\n",
        "              \"randomforestclassifier__min_samples_leaf\": np.arange(1, 8, 1),\n",
        "              \"randomforestclassifier__max_leaf_nodes\": np.arange(2750, 3250, 50),\n",
        "              \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "model_cv = RandomizedSearchCV(model_rf, param_distributions=param_rngs, n_iter=100, cv=5, n_jobs=-1, random_state=42, verbose=1, scoring='accuracy')\n",
        "\n",
        "#model_cv = GridSearchCV(model_rf, param_grid=param_rngs, cv=5, n_jobs=-1, random_state=42, verbose=1, scoring='accuracy')\n",
        "\n",
        "model_cv.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWflh5pH2DoP"
      },
      "source": [
        "**Task 8:** Print out the best score and best params for `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZP8RZlPD2DoP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score for `model`: 0.8061196008100934\n",
            "Best params for `model`: {'simpleimputer__strategy': 'median', 'randomforestclassifier__n_estimators': 65, 'randomforestclassifier__min_samples_split': 7, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__max_leaf_nodes': 2900, 'randomforestclassifier__max_features': 0.42500000000000004, 'randomforestclassifier__max_depth': 32, 'randomforestclassifier__criterion': 'entropy'}\n"
          ]
        }
      ],
      "source": [
        "best_score = model_cv.best_score_\n",
        "#best_score = accuracy_score(y, model_cv.predict(X))\n",
        "best_params = model_cv.best_params_\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6DghX_-2DoP"
      },
      "source": [
        "# Communicate Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLst3oyi2DoQ"
      },
      "source": [
        "**Task 9:** Create a DataFrame `submission` whose index is the same as `X_test` and that has one column `'status_group'` with your predictions. Next, save this DataFrame as a CSV file and upload your submissions to our competition site. \n",
        "\n",
        "**Note:** Check the `sample_submission.csv` file on the competition website to make sure your submissions follows the same formatting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "x4FPqRfI2DoQ"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(data=model_cv.predict(X_test), index=X_test.index)\n",
        "submission.columns = ['status_group']\n",
        "\n",
        "# generate CSV\n",
        "submission.to_csv('submission_jd_3.csv')\n",
        "# download\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import files\n",
        "    files.download(\"submission_jd_3.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LS_DS_223_assignment_JD.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "687c238b3ce093b6d05593c14198ba6d4a27c356ca33aff629c1e2e7517a29f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
